### 2.2.1 Create tables

It is possible to create Delta tables using **Notebooks** (see Next) that will be stored in the *Tables* folder of the Lakehouse. 
Using **Python** (*PySpark*), some examples of code for read/write data are the following:

```python
# Load a file and store data into a dataframe (using PySpark)
df = spark.read.load('Files/mydata.csv', format = 'csv', header = True)
df = spark.read.csv('path', inferSchema = True)
df = spark.read.parquet('Files/Sub_folder/table.parquet')
df = spark.read.parquet('Files/*.parquet') 	# Combine parquet files with the same schema 
df = spark.read.json('Files/Sub_folder/json_file')

# Python
df = pd.read_parquet('/lakehouse/default/' + 'Files/Sub_folder/table.parquet') 
				 
# From Spark to Pandas
df_pandas = df.toPandas()				

#######################################################################################################
# Save the dataframe as a delta table
# MANAGED TABLE (in Tables)
df.write.format('delta').saveAsTable('mytable')

# EXTERNAL TABLE (in Files)
df.write.format('delta').saveAsTable('mytable', path = 'Files/folder')

# In a specified path
df.write.format('delta').save(path)		

# Other type of files
df.write.json('path', mode = ...)	 # Specify the writing mode (e.g. append, overwrite,...)
df.write.parquet('path', mode = ...)
```

**Pay attention: this is the way to save data when dealing with a Spark dataframe!** For Pandas *dataframes*, some other functions are required (for example: *to_csv, to_xlsx, to_parquet, to_json*)! For Pandas dataframes, the ***ABFS path*** of the final folder must be used in the *destination* path string (it is possible to retrieve the path from the folder itself – left menu and three dots). 

> *For CSV files, the **inferSchema** option will let Spark identify the table schema. Otherwise, everything is read as “string” (see next to define a Schema).*

In general, it is possible to *drag & drop* the table into the Notebook to generate an automatic piece of code. Reading a file in this way may set a wrong data type for the table columns (i.e., string type for all). To change data type, this code can be used (with the function ***cast***):

```python
# Utilities
from pyspark.sql.functions import col, to_timestamp
from pyspark.sql.types import IntegerType, DateType, FloatType

df = df.withColumn("column_name", col("column_name").cast(IntegerType()))		#Integers
df = df.withColumn("column_name", col("column_name").cast(FloatType()))			#Floats   
df = df.withColumn("column_name", to_date(col("column_name")).cast(DateType()))		#Dates
df = df.withColumn("column_name", to_timestamp(col("column_name"))			#DateTime  
df = df.withColumn("column_name", col("datetype_column_name").cast(TimestampType()))	#DateTime  
```

> The function ***withColumn*** is used to define new columns or to modify existing ones:

> ```python
> df.withColumn("name", expression)
> ```

Alternatively, it is possible to define a ***schema*** before reading a file, using a code like the following:

```python
from pyspark.sql.types import * 			# Import data types
From pyspark.sql.functions import * 			# Import useful functions

# Define schema
productSchema = StructType([ 
	StructField("ProductName", StringType(), True),
	StructField("ListPrice", FloatType(), False),
	...
]) 

# Use the schema while reading data into a dataframe
df = spark.read.load('/data/product-data.csv', format = 'csv', schema = productSchema, ...)

# To create a dataframe from data
df = spark.createDataFrame(data = your_data, schema = productSchema)

# To print the schema of a dataframe (columns, data types, true/false for nulls)
df.printSchema()
```

With schemas, Spark will not “infer” the data type on its own (which can be **expensive** in term of resources). 
Moreover, errors can be detected faster when something does not match a schema. 
The *True/False* at the end of each column in the schema is for accepting null values. 

In this context, it is possible to extract the *Spark schema* from a dataframe and use it to create a new dataframe with the same Spark schema (without re-defining it):

```python
# Expression to get the Schema from a dataframe
df.schema()
```

.

In a Notebook, it is also possible to use **SQL** to create two kinds of tables:

```sql
-- MANAGED TABLE
%%sql
CREATE TABLE salesorders
(
   Orderid INT NOT NULL,
   OrderDate TIMESTAMP NOT NULL,
   CustomerName STRING,
   SalesTotal FLOAT NOT NULL
)
USING DELTA
AS SELECT … (add fields) … FROM … (add table) …


-- EXTERNAL TABLE
%%sql
CREATE TABLE MyExternalTable (
   -- also with schema
   ...
)
USING DELTA
LOCATION 'Files/mydata'
AS SELECT ... (add fields) ... FROM ... (add table) ...
```

In the first case (and the Python example), the produced table is a ***Managed table***: data and metadata are both managed by Spark in the Lakehouse – if the table is deleted, everything will be deleted. In the second case, the produced table is an ***External table***: the table definition is in the *Tables* folder but data, in Parquet format, is stored in the *Files* folder; if the table is deleted, data will be preserved in the *Files* folder. 

> Important: external tables are not visible in the SQL Endpoint of the Lakehouse!

**Within a Notebook, it is possible to write a query to modify and/or add new items to an existing table, using SQL** (it is not possible to do it from the Lakehouse *SQL EndPoint* because it is **read-only**!):

```sql
%%sql
ALTER TABLE table_name ADD COLUMN col_name int

UPDATE table_name SET col_name = 10
```

Data can be **overwritten** or **appended**. To do that using PySpark, one must use a script like the following:

```python
new_df.write.format("delta").mode("overwrite").save(path) 		# Overwrite
new_rows_df.write.format("delta").mode("append").save(path)   		# Append    
```

In general, there are other two modes to write tables:

- *error mode*: it throws an error if data already exists.
- *ignore mode*: it fails if data already exists. It is possible to handle the case.

Finally, it is possible to store data in the *Files* folders with ***partitions***. To do that, the syntax is the following:

```python
# Partitions
df.write.partitionBy('col_date', 'col_month', ...).mode('overwrite').parquet('Files/Partition_folder')
```

In this way, the dataframe will be splitted based on the columns defined in *partitionBy*, and Spark automatically creates **folders** and **subfolders** whose names contain Year, Month, and other specified values.
