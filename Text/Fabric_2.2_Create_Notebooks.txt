### 2.2 Create Notebooks

***Notebooks*** are a very useful tool to perform specific operations on data using programming languages like Python or R.
When a new notebook is created, in the main hub, it is possible to select the language (default: *PySpark - Python*); alternatively, the language can be changed inside the script by using the so-called *magic command*: the syntax is ***%%language_name***.

> *Spark exists to clean, transform, analyze, and make prediction on **big data**. Spark distributes big datasets analyzed in a Spark session on several servers (i.e. it splits the dataset). It performs multiple tasks in parallel (on each dataset split) making it very fast. It is possible to use 5 languages in the Spark Core & Spark SQL Engine (Scala, SQL, Python, Java, R). On top of the Spark Core & Spark SQL Engine, there are 4 libraries:*
>  - *Spark SQL end Dataframes (to work with tables and dataframe, like Pandas).*
>  - *Spark Streaming (for real-time/streaming data).*
>  - *Machine Learning MLib (at the heart of the Data Science experience, along with other libraries).*
>  - *Graph Processing (Graph X - for graph database).*

The main commands of the notebook are exactly those used in **Visual Studio Code** (e.g. to create markdown cells, to create/delte cells, to run cells, etc.). It is possible to connect a Notebook to an existing *Lakehouse* using the appropriate menu on the left. Here, it is also possible to manage ***resources*** (e.g., *.py* files with functions, codes, images, etc. up to 500 MB)

>> ***Notes on resources: missing***

A query inside a notebook can be visualized as a *table* or as a *chart*, by selecting the appropriate button in the cell output. When "chart" is selected, it is possible to choose the visualization chart and the set of fields and properties in the right pane.[^1]

If a notebook is ready, in the *Settings* section (top left menu) it is possible to choose a name, set the sensitivity label, or schedule an execution time. Once saved, a notebook can be used by going in "*Open notebook*" and selecting "*Existing notebook*" (a list will appear).

When using notebooks, it is crucial to avoid *pip install* because it consumes resources and makes the notebook run longer. To avoid that, it is possible to define an ***Environment*** (*preview*), even from a notebook, where libraries (even custom ones - only *.whl* or *.jar* files) can be handled and defined. In the Workspace Settings, go to "Data Engineering/Science", then "Spark settings", and finally "*Environment*": here, it is possible to see detauls about libraries. When an environment is defined, it is not necessary to install libraries each time a notebook runs.[^2]




[^1]: In a Notebook, in section “Run”, the default option is “*Standard Session*”. It is possible to run multiple notebooks (up to 5) by selecting “**New high concurrency session**”: after the initialization of the session in one notebook, it is possible to open a new notebook and in the “Run” session, select the existing high concurrency session to run the two notebooks together.

[^2]: An *environment* can be used in different Notebooks.
