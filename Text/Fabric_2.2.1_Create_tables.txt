### 2.2.1 Create tables

It is possible to create Delta tables using **Notebooks** (see Next) that will be stored in the *Tables* folder of the Lakehouse. 
Using **Python** (*PySpark*), some examples of code for read/write data are the following:

```python
# Load a file and store data into a dataframe (using PySpark)
df = spark.read.load('Files/mydata.csv', format = 'csv', header = True)
df = spark.read.csv('path', inferSchema = True)
df = spark.read.parquet("Files/Sub_folder/table.parquet")
df = spark.read.parquet("Files/*.parquet") 	# Combine parquet files with the same schema 
df = spark.read.json("Files/Sub_folder/json_file")

# Python
df = pd.read_parquet("/lakehouse/default/"+"Files/Sub_folder/table.parquet") 
				 
# From Spark to Pandas
df_pandas = df.toPandas()				

#######################################################################################################
# Save the dataframe as a delta table
# MANAGED TABLE (in Tables)
df.write.format("delta").saveAsTable("mytable")

# EXTERNAL TABLE (in Files)
df.write.format("delta").saveAsTable("mytable", path=”Files/folder”)

# In a specified path
df.write.format("delta").save(path)		

# Other type of files
df.write.json("path", mode = ...)	 # Specify the writing mode (e.g. append, overwrite,...)
df.write.parquet("path", mode = ...)
```

**Pay attention: this is the way to save data when dealing with a Spark dataframe!** For Pandas *dataframes*, some other functions are required (for example: *to_csv, to_xlsx, to_parquet, to_json*)! For Pandas dataframes, the ***ABFS path*** of the final folder must be used in the *destination* path string (it is possible to retrieve the path from the folder itself – left menu and three dots). 

>> *For CSV files, the **inferSchema** option will let Spark identify the table schema. Otherwise, everything is read as “string” (see next to define a Schema).*

In general, it is possible to *drag & drop* the table into the Notebook to generate an automatic piece of code. Reading a file in this way may set a wrong data type for the table columns (i.e., string type for all). To change data type, this code can be used (with the function ***cast***):

```python
# Utilities
from pyspark.sql.functions import col, to_timestamp
from pyspark.sql.types import IntegerType, DateType, FloatType

df = df.withColumn("column_name", col("column_name").cast(IntegerType()))		#Integers
df = df.withColumn("column_name", col("column_name").cast(FloatType()))			#Floats   
df = df.withColumn("column_name", to_date(col("column_name")).cast(DateType()))		#Dates
df = df.withColumn("column_name", to_timestamp(col("column_name))			#DateTime  
df = df.withColumn("column_name", col("datetype_column_name).cast(TimestampType()))	#DateTime  
```

>> The function ***withColumn*** is used to define new columns or to modify existing ones:

>> ```python
>> df.withColumn("name", expression)
>> ```

Alternatively, it is possible to define a ***schema*** before reading a file, using a code like the following:
