It is possible to create Delta tables using **Notebooks** (see Next) that will be stored in the *Tables* folder of the Lakehouse. 
Using **Python** (*PySpark*), some examples of code for read/write data are the following:

```python
from pyomo.environ import *
model = Concretemodel()

# Load a file into a dataframe
df = spark.read.load('Files/mydata.csv', format='csv', header=True)
df = spark.read.csv('path', inferSchema = True)
df = spark.read.parquet("Files/Sub_folder/table.parquet")
df = spark.read.parquet("Files/*.parquet") 	#Combine parquet files with the same schema 
df = spark.read.json("Files/Sub_folder/json_file")

#Python
df = pd.read_parquet("/lakehouse/default/"+"Files/Sub_folder/table.parquet") 
				 
# If I want to use Pandas dataframe
df_pandas = df.toPandas()				

##########################################################################################
# Save the dataframe as a delta table
# MANAGED TABLE (in Tables)
df.write.format("delta").saveAsTable("mytable")

# EXTERNAL TABLE (in Files)
df.write.format("delta").saveAsTable("mytable", path=”Files/folder”)

# In the specified path
df.write.format("delta").save(path)		

# Other type of files
df.write.json("path", mode = …)
df.write.parquet("path", mode = …)
```
