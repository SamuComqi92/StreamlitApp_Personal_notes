### Power Query, dataflows vs. Microsoft Fabric notebooks
#### A simple table-join case analysis

*Problem statement*

> *The operation is to join two tables whose key columns had a ***high cardinality*** (with a many-to-many relationship) and whose values were created as a concatenation of three numeric codes separated by “||” (i.e. each value was like “61003952||60013161||20240302”).*
> *The operation was a simple **inner join**.*
> *A **inner join** is also the required operation to replicate the Qlik ***where exists*** clause.*
> *The key-columns of the two tables have ~3mln and ~4mln distinct values.*

Tools and services used to conduct the tests:
- **Power Query** (online on Power BI Service using a Dataflow, and on Power BI Desktop).
- A Microsoft Fabric **Lakehouse** (created to store the source tables to be joined).
- A Microsoft Fabric **Warehouse** (created to use *T-SQL* to join the two tables and create a new one).
- A Microsoft Fabric **Notebook** (to use *PySpark* to perform the join operation).

.

A. ***Power Query (on Desktop and online)***

Using both the Power Query editor on Power BI Desktop and Power Query Online in a *Dataflow* to perform the join operation, the estimation of matches did not show anything, even after a long waiting time.
During the evaluation process, no data preview was visible, and eventually, a message appeared in the dataflow: "*Something went wrong while evaluating this query*".
In Power BI Desktop, after forcing the editor to apply the changes (without waiting for the preview to show up), the loading process was stuck in an endless “*Evaluating...*”.

Reasons:
- *Power query is powerful but not so powerful*: the editor is good for simple operations and transformations and is well suited for relatively small tables. When merges and complex transformations join the party, the classic statement is “Push everything in the back-end” i.e. let another tool do the job and use Power BI only to read the final result to avoid additional heavy operation.
- *String key columns*: in general, string columns have a larger size compared to numeric ones. The Power Query editor on Power BI Desktop does not compress data, therefore it “feels” the columns’ size resulting in a very slow join operation (*data compression occurs later, after loading the data, when the VertiPaq storage engine compresses everything and creates dictionaries for fast reading and data retrieval*).
- *High cardinality*: having a large number of distinct values means that Power Query needs to scan a larger number of strings and make a larger number of comparisons. .

.

B. ***CTAS in Fabric Warehouse using T-SQL***

In the Fabric Warehouse, the following CTAS was successful in creating the joined table (the two tables were stored in the Lakehouse):

```sql
CREATE TABLE [dbo.new_table] AS
SELECT
*
FROM [lakehouse].[dbo].[table_a]        a
INNER JOIN [lakehouse].[dbo].[table_b]  b
ON a.key = b.key
```

It took 6 minutes to create a big table of ~48 million rows without being stuck in an endless operation.

.

C. ***Fabric Notebook & PySpark***

A Notebook was connected to the Lakehouse containing the two tables. A pair of tests were conducted:

1. The two tables were uploaded first. After that, they have been merged and the final table stored in the Lakehouse as a *Delta table*:

```python
table_a = spark.sql('SELECT * FROM lakehouse.table_a')
table_b = spark.sql('SELECT * FROM lakehouse.table_b')

joined_df = table_a.join(table_b, on = "key", how = "inner")

joined_df.write.mode("overwrite").format("delta").saveAsTable("joined_table")
```

Combining everything (i.e. tables upload, merge, storage), the total time was 5 minutes and a half.
A comparable timing (i.e. ~6 minutes) was obtained by running a more explicit query:

```python
joined_df = spark.sql("""SELECT
*
FROM lakehouse_table_a        a
INNER JOIN lakehouse.table_b  b
ON a.key = b.key
""")

joined_df.write.mode("overwrite").format("delta").saveAsTable("joined_table")
```

.

.

***Qlik “where exists” using T-SQL***

The translation of the Qlik *where exists* clause corresponds to an **inner join** that retrieves only the values already present in another column of another loaded table. 
For the test, key-columns were always complex string-type but with a lower cardinality (i.e. 350k distinct values in the left table, and 3k in the right table).

Power Query in Power BI Desktop and Online (in a dataflow) was unsuccessful and not able to perform the join (not even showing the preview).

Instead, the following **T-SQL** script was incredibly fast:

```sql
CREATE TABLE [dbo.new_table] AS
SELECT
a.*
FROM [lakehouse].[dbo].[table_a]    a
WHERE EXISTS (
  SELECT 1
  FROM [lakehouse].[dbo].[table_b]  b
  WHERE a.key = b.key
)
```

It took 3 seconds to produce a small table of ~22k rows (i.e. values both in *table_a* and *table_b*).

.

***Qlik “where exists” using PySpark***

Doing a normal inner join will produce a bigger table because the operation will retrieve several rows from the right table for each key in the left table. To reproduce the *where exists* operation, it is necessary to use the *dropDuplicates()* on the joined table. The final result was achieved in ~20 seconds:

```python
# Upload tables
table_a = spark.sql('SELECT * FROM lakehouse.table_a')
table_b = spark.sql('SELECT * FROM lakehouse.table_b')

# Inner join
joined_df = table_a.join(table_b, on = "key", how = "inner")

# Remove duplicates
joined_df = joined_df.dropDuplicates()

# Save results
joined_df.write.mode("overwrite").format("delta").saveAsTable("joined_table")
```

However, using an explicit SQL query (without loading the tables) led to a faster response (less than 10 seconds):

```python
joined_df = spark.sql("""SELECT
a.*
FROM [lakehouse].[dbo].[table_a]    a
WHERE EXISTS (
  SELECT 1
  FROM [lakehouse].[dbo].[table_b]  b
  WHERE a.key = b.key
)

# Save results
joined_df.write.mode("overwrite").format("delta").saveAsTable("joined_table")
```

.
.

**Conclusions**

- Power Query is good for simple transformations (e.g. renaming, creating a new column with a simple logic, replacing values), both in Power BI Desktop and a Dataflow. Moreover, some of those operations can lead to ***query folding***, “pushing” everything to the source to optimize the query execution.
- Power Query can’t handle complex transformation operations, such as *joining tables* (such operations are not “pushed” to the source by the *query folding* process), especially when dealing with very large tables and *high cardinality* columns (of the order of millions of values). For this reason, the best practice is to let the back-end handle those operations and leave only the final step (i.e. reading the table) to Power BI.
- **Microsoft Fabric might be the solution to solve many problems regarding those complex operations**, using both *T-SQL* (in a Warehouse) and/or *PySpark* (in a Notebook), combining millions of values and in a small amount of time.
